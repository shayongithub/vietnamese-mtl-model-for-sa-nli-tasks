{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyNNNW6OSFrMTwbL1tkt+O9N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uQW6QFBZHF-q","executionInfo":{"status":"ok","timestamp":1692607903577,"user_tz":-420,"elapsed":19197,"user":{"displayName":"Project CSE","userId":"17723469996248471467"}},"outputId":"04f3ad13-8f61-4b46-da02-0e73c7998de8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install accelerate==0.21.0 transformers==4.31.0 datasets==2.14.0 evaluate==0.4.0 loguru seqeval"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JqfbspyCHJmF","executionInfo":{"status":"ok","timestamp":1692607919371,"user_tz":-420,"elapsed":15797,"user":{"displayName":"Project CSE","userId":"17723469996248471467"}},"outputId":"007115ca-34a3-4563-b84e-e0816663016e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting accelerate==0.21.0\n","  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers==4.31.0\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets==2.14.0\n","  Downloading datasets-2.14.0-py3-none-any.whl (492 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.2/492.2 kB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting evaluate==0.4.0\n","  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting loguru\n","  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.31.0)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.31.0)\n","  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (4.66.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.0) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets==2.14.0)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.0) (1.5.3)\n","Collecting xxhash (from datasets==2.14.0)\n","  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets==2.14.0)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.0) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.0) (3.8.5)\n","Collecting responses<0.19 (from evaluate==0.4.0)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.0) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.0) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.0) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.0) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.0) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.0) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.0) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.7.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2023.7.22)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.21.0) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.21.0) (16.0.6)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.0) (2023.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.14.0) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=3b841b6bf12a89cd53b1c4d614f1c29199af6784f8d9710a07cbbceb5fbe911a\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","Successfully built seqeval\n","Installing collected packages: tokenizers, safetensors, xxhash, loguru, dill, responses, multiprocess, huggingface-hub, transformers, seqeval, datasets, evaluate, accelerate\n","Successfully installed accelerate-0.21.0 datasets-2.14.0 dill-0.3.7 evaluate-0.4.0 huggingface-hub-0.16.4 loguru-0.7.0 multiprocess-0.70.15 responses-0.18.0 safetensors-0.3.2 seqeval-1.2.2 tokenizers-0.13.3 transformers-4.31.0 xxhash-3.3.0\n"]}]},{"cell_type":"markdown","source":["## **Sentiment Analysis**"],"metadata":{"id":"nAdoHm3AIRFx"}},{"cell_type":"markdown","source":["### **ViDeBERTa-xmall**"],"metadata":{"id":"i41Q1981IYAO"}},{"cell_type":"code","source":["!python /content/drive/MyDrive/Shay/training_script/trainer_sa_colab.py --save_path /content/drive/MyDrive/Shay/training_script/output/models/sa_videberta_xsmall_model_10_epochs_21082023 --checkpoint_dir /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023 --encoder_name Fsoft-AIC/videberta-xsmall --max_seq_length 512 --per_device_train_batch_size 64 --per_device_eval_batch_size 64 --logging_steps 1000 --num_train_epochs 10 --save_strategy epoch --remove_unused_columns --load_best_model_at_end"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2l81NfjUImF0","executionInfo":{"status":"ok","timestamp":1692608229119,"user_tz":-420,"elapsed":285763,"user":{"displayName":"Project CSE","userId":"17723469996248471467"}},"outputId":"080d485f-7eeb-4e71-900b-7ff80325b77d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-08-21 08:52:34.036494: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Downloading builder script: 100% 4.20k/4.20k [00:00<00:00, 16.8MB/s]\n","Downloading builder script: 100% 6.77k/6.77k [00:00<00:00, 21.3MB/s]\n","Downloading builder script: 100% 7.55k/7.55k [00:00<00:00, 26.1MB/s]\n","Downloading builder script: 100% 7.36k/7.36k [00:00<00:00, 27.1MB/s]\n","\u001b[32m2023-08-21 08:52:43.401\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m437\u001b[0m - \u001b[32m\u001b[1mPyTorch is running on GPU: NVIDIA A100-SXM4-40GB\u001b[0m\n","\u001b[32m2023-08-21 08:52:43.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m481\u001b[0m - \u001b[1m***** Running training arguments *****\u001b[0m\n","\u001b[32m2023-08-21 08:52:43.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m482\u001b[0m - \u001b[1m  Encoder name = Fsoft-AIC/videberta-xsmall\u001b[0m\n","\u001b[32m2023-08-21 08:52:43.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m484\u001b[0m - \u001b[1m  Max sequence length = 512\u001b[0m\n","\u001b[32m2023-08-21 08:52:43.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m485\u001b[0m - \u001b[1m  Max train samples = None\u001b[0m\n","\u001b[32m2023-08-21 08:52:43.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m486\u001b[0m - \u001b[1m  Max evaluation samples = None\u001b[0m\n","\u001b[32m2023-08-21 08:52:43.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m488\u001b[0m - \u001b[1m  Num Epochs = 10\u001b[0m\n","\u001b[32m2023-08-21 08:52:43.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m489\u001b[0m - \u001b[1m  Training batch size per device = 64\u001b[0m\n","\u001b[32m2023-08-21 08:52:43.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m492\u001b[0m - \u001b[1m  Evaluation batch size per device = 64\u001b[0m\n","\u001b[32m2023-08-21 08:52:43.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m495\u001b[0m - \u001b[1m  Checkpoint dir = /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023\u001b[0m\n","\u001b[32m2023-08-21 08:52:43.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m496\u001b[0m - \u001b[1m  Save Strategy = epoch\u001b[0m\n","\u001b[32m2023-08-21 08:52:43.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m497\u001b[0m - \u001b[1m  Number of save steps = 3,000\u001b[0m\n","\u001b[32m2023-08-21 08:52:43.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m498\u001b[0m - \u001b[1m  Number of logging steps = 1,000\u001b[0m\n","\u001b[32m2023-08-21 08:52:43.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m499\u001b[0m - \u001b[1m  Learning rate = 5e-05\u001b[0m\n","\u001b[32m2023-08-21 08:52:43.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m500\u001b[0m - \u001b[1m  Weight decay = 0.01\u001b[0m\n","\u001b[32m2023-08-21 08:52:43.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m501\u001b[0m - \u001b[1m  Load best model at end = False\u001b[0m\n","\u001b[32m2023-08-21 08:52:43.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m503\u001b[0m - \u001b[1m  Remove unused columns = True\u001b[0m\n","\u001b[32m2023-08-21 08:52:43.409\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m327\u001b[0m - \u001b[33m\u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n","\u001b[32m2023-08-21 08:52:43.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1mTraining/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=True,\n","do_eval=True,\n","do_predict=False,\n","do_train=False,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=True,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=True,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/runs/Aug21_08-52-43_d3c591d2248e,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=1000,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=f1,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=10,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=/content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=64,\n","per_device_train_batch_size=64,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=3000,\n","save_strategy=epoch,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.01,\n","xpu_backend=None,\n",")\u001b[0m\n","Downloading (…)okenizer_config.json: 100% 401/401 [00:00<00:00, 2.48MB/s]\n","Downloading (…)/main/tokenizer.json: 100% 8.49M/8.49M [00:00<00:00, 8.59MB/s]\n","Downloading (…)cial_tokens_map.json: 100% 173/173 [00:00<00:00, 1.10MB/s]\n","[INFO|tokenization_utils_base.py:1839] 2023-08-21 08:52:46,652 >> loading file spm.model from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-08-21 08:52:46,652 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Fsoft-AIC--videberta-xsmall/snapshots/029b6b1b1d7636221bd2d1a98b46a2fdba3af34e/tokenizer.json\n","[INFO|tokenization_utils_base.py:1839] 2023-08-21 08:52:46,652 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1839] 2023-08-21 08:52:46,652 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--Fsoft-AIC--videberta-xsmall/snapshots/029b6b1b1d7636221bd2d1a98b46a2fdba3af34e/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:1839] 2023-08-21 08:52:46,652 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Fsoft-AIC--videberta-xsmall/snapshots/029b6b1b1d7636221bd2d1a98b46a2fdba3af34e/tokenizer_config.json\n","Map:   0% 0/12478 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:2471] 2023-08-21 08:52:52,757 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Map: 100% 12478/12478 [00:00<00:00, 12888.77 examples/s]\n","Map: 100% 2999/2999 [00:00<00:00, 18587.39 examples/s]\n","Downloading (…)lve/main/config.json: 100% 609/609 [00:00<00:00, 3.18MB/s]\n","[INFO|configuration_utils.py:712] 2023-08-21 08:52:54,425 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Fsoft-AIC--videberta-xsmall/snapshots/029b6b1b1d7636221bd2d1a98b46a2fdba3af34e/config.json\n","[INFO|configuration_utils.py:768] 2023-08-21 08:52:54,430 >> Model config DebertaV2Config {\n","  \"_name_or_path\": \"Fsoft-AIC/videberta-xsmall\",\n","  \"attention_head_size\": 64,\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 6,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 384,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128000\n","}\n","\n","Downloading pytorch_model.bin: 100% 241M/241M [00:00<00:00, 348MB/s]\n","[INFO|modeling_utils.py:2603] 2023-08-21 08:52:56,088 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--Fsoft-AIC--videberta-xsmall/snapshots/029b6b1b1d7636221bd2d1a98b46a2fdba3af34e/pytorch_model.bin\n","[INFO|modeling_utils.py:3319] 2023-08-21 08:52:56,890 >> Some weights of the model checkpoint at Fsoft-AIC/videberta-xsmall were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'deberta.embeddings.word_embeddings._weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:3331] 2023-08-21 08:52:56,891 >> Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at Fsoft-AIC/videberta-xsmall and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|trainer.py:763] 2023-08-21 08:53:01,231 >> The following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:1686] 2023-08-21 08:53:01,241 >> ***** Running training *****\n","[INFO|trainer.py:1687] 2023-08-21 08:53:01,242 >>   Num examples = 12,478\n","[INFO|trainer.py:1688] 2023-08-21 08:53:01,242 >>   Num Epochs = 10\n","[INFO|trainer.py:1689] 2023-08-21 08:53:01,242 >>   Instantaneous batch size per device = 64\n","[INFO|trainer.py:1692] 2023-08-21 08:53:01,242 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n","[INFO|trainer.py:1693] 2023-08-21 08:53:01,242 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1694] 2023-08-21 08:53:01,242 >>   Total optimization steps = 1,950\n","[INFO|trainer.py:1695] 2023-08-21 08:53:01,243 >>   Number of trainable parameters = 70,792,322\n","[WARNING|logging.py:280] 2023-08-21 08:53:01,274 >> You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","[INFO|trainer.py:2807] 2023-08-21 08:53:26,012 >> Saving model checkpoint to /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-195\n","[INFO|configuration_utils.py:458] 2023-08-21 08:53:26,018 >> Configuration saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-195/config.json\n","[INFO|modeling_utils.py:1851] 2023-08-21 08:53:26,702 >> Model weights saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-195/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2210] 2023-08-21 08:53:26,706 >> tokenizer config file saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-195/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2217] 2023-08-21 08:53:26,710 >> Special tokens file saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-195/special_tokens_map.json\n","[INFO|trainer.py:2807] 2023-08-21 08:53:50,068 >> Saving model checkpoint to /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-390\n","[INFO|configuration_utils.py:458] 2023-08-21 08:53:50,073 >> Configuration saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-390/config.json\n","[INFO|modeling_utils.py:1851] 2023-08-21 08:53:50,765 >> Model weights saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-390/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2210] 2023-08-21 08:53:50,769 >> tokenizer config file saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-390/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2217] 2023-08-21 08:53:50,772 >> Special tokens file saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-390/special_tokens_map.json\n","[INFO|trainer.py:2807] 2023-08-21 08:54:14,528 >> Saving model checkpoint to /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-585\n","[INFO|configuration_utils.py:458] 2023-08-21 08:54:14,533 >> Configuration saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-585/config.json\n","[INFO|modeling_utils.py:1851] 2023-08-21 08:54:15,213 >> Model weights saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-585/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2210] 2023-08-21 08:54:15,217 >> tokenizer config file saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-585/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2217] 2023-08-21 08:54:15,231 >> Special tokens file saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-585/special_tokens_map.json\n","[INFO|trainer.py:2807] 2023-08-21 08:54:38,385 >> Saving model checkpoint to /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-780\n","[INFO|configuration_utils.py:458] 2023-08-21 08:54:38,391 >> Configuration saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-780/config.json\n","[INFO|modeling_utils.py:1851] 2023-08-21 08:54:39,064 >> Model weights saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-780/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2210] 2023-08-21 08:54:39,067 >> tokenizer config file saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-780/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2217] 2023-08-21 08:54:39,070 >> Special tokens file saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-780/special_tokens_map.json\n","[INFO|trainer.py:2807] 2023-08-21 08:55:02,249 >> Saving model checkpoint to /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-975\n","[INFO|configuration_utils.py:458] 2023-08-21 08:55:02,255 >> Configuration saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-975/config.json\n","[INFO|modeling_utils.py:1851] 2023-08-21 08:55:02,949 >> Model weights saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-975/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2210] 2023-08-21 08:55:02,953 >> tokenizer config file saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-975/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2217] 2023-08-21 08:55:02,956 >> Special tokens file saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-975/special_tokens_map.json\n","{'loss': 0.2039, 'learning_rate': 2.435897435897436e-05, 'epoch': 5.13}\n","[INFO|trainer.py:2807] 2023-08-21 08:55:26,064 >> Saving model checkpoint to /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1170\n","[INFO|configuration_utils.py:458] 2023-08-21 08:55:26,069 >> Configuration saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1170/config.json\n","[INFO|modeling_utils.py:1851] 2023-08-21 08:55:26,769 >> Model weights saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1170/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2210] 2023-08-21 08:55:26,773 >> tokenizer config file saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1170/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2217] 2023-08-21 08:55:26,775 >> Special tokens file saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1170/special_tokens_map.json\n","[INFO|trainer.py:2807] 2023-08-21 08:55:51,734 >> Saving model checkpoint to /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1365\n","[INFO|configuration_utils.py:458] 2023-08-21 08:55:51,740 >> Configuration saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1365/config.json\n","[INFO|modeling_utils.py:1851] 2023-08-21 08:55:52,433 >> Model weights saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1365/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2210] 2023-08-21 08:55:52,438 >> tokenizer config file saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1365/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2217] 2023-08-21 08:55:52,446 >> Special tokens file saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1365/special_tokens_map.json\n","[INFO|trainer.py:2807] 2023-08-21 08:56:15,693 >> Saving model checkpoint to /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1560\n","[INFO|configuration_utils.py:458] 2023-08-21 08:56:15,698 >> Configuration saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1560/config.json\n","[INFO|modeling_utils.py:1851] 2023-08-21 08:56:16,371 >> Model weights saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1560/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2210] 2023-08-21 08:56:16,375 >> tokenizer config file saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1560/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2217] 2023-08-21 08:56:16,392 >> Special tokens file saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1560/special_tokens_map.json\n","[INFO|trainer.py:2807] 2023-08-21 08:56:39,882 >> Saving model checkpoint to /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1755\n","[INFO|configuration_utils.py:458] 2023-08-21 08:56:39,888 >> Configuration saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1755/config.json\n","[INFO|modeling_utils.py:1851] 2023-08-21 08:56:40,590 >> Model weights saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1755/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2210] 2023-08-21 08:56:40,595 >> tokenizer config file saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1755/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2217] 2023-08-21 08:56:40,598 >> Special tokens file saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1755/special_tokens_map.json\n","[INFO|trainer.py:2807] 2023-08-21 08:57:03,896 >> Saving model checkpoint to /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1950\n","[INFO|configuration_utils.py:458] 2023-08-21 08:57:03,901 >> Configuration saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1950/config.json\n","[INFO|modeling_utils.py:1851] 2023-08-21 08:57:04,582 >> Model weights saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1950/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2210] 2023-08-21 08:57:04,585 >> tokenizer config file saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1950/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2217] 2023-08-21 08:57:04,588 >> Special tokens file saved in /content/drive/MyDrive/Shay/training_script/output/runs/sa_videberta_xsmall_runs_10_epochs_21082023/checkpoint-1950/special_tokens_map.json\n","[INFO|trainer.py:1934] 2023-08-21 08:57:06,014 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 244.7719, 'train_samples_per_second': 509.781, 'train_steps_per_second': 7.967, 'train_loss': 0.13964745545998597, 'epoch': 10.0}\n","[INFO|trainer.py:2807] 2023-08-21 08:57:06,023 >> Saving model checkpoint to /content/drive/MyDrive/Shay/training_script/output/models/sa_videberta_xsmall_model_10_epochs_21082023\n","[INFO|configuration_utils.py:458] 2023-08-21 08:57:06,028 >> Configuration saved in /content/drive/MyDrive/Shay/training_script/output/models/sa_videberta_xsmall_model_10_epochs_21082023/config.json\n","[INFO|modeling_utils.py:1851] 2023-08-21 08:57:06,746 >> Model weights saved in /content/drive/MyDrive/Shay/training_script/output/models/sa_videberta_xsmall_model_10_epochs_21082023/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2210] 2023-08-21 08:57:06,752 >> tokenizer config file saved in /content/drive/MyDrive/Shay/training_script/output/models/sa_videberta_xsmall_model_10_epochs_21082023/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2217] 2023-08-21 08:57:06,755 >> Special tokens file saved in /content/drive/MyDrive/Shay/training_script/output/models/sa_videberta_xsmall_model_10_epochs_21082023/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =       10.0\n","  train_loss               =     0.1396\n","  train_runtime            = 0:04:04.77\n","  train_samples            =      12478\n","  train_samples_per_second =    509.781\n","  train_steps_per_second   =      7.967\n","\u001b[32m2023-08-21 08:57:06.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m409\u001b[0m - \u001b[1mMetrics: {'train_runtime': 244.7719, 'train_samples_per_second': 509.781, 'train_steps_per_second': 7.967, 'train_loss': 0.13964745545998597, 'epoch': 10.0, 'train_samples': 12478}\u001b[0m\n","\u001b[32m2023-08-21 08:57:08.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m415\u001b[0m - \u001b[1m*** Evaluate Validation dataset ***\u001b[0m\n","[INFO|trainer.py:763] 2023-08-21 08:57:08,764 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3081] 2023-08-21 08:57:08,766 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3083] 2023-08-21 08:57:08,766 >>   Num examples = 2999\n","[INFO|trainer.py:3086] 2023-08-21 08:57:08,766 >>   Batch size = 64\n","{'eval_loss': 0.26157641410827637, 'eval_accuracy': 0.9373124374791597, 'eval_precision': 0.9540155440414507, 'eval_recall': 0.9264150943396227, 'eval_f1': 0.9400127632418633, 'eval_runtime': 2.0136, 'eval_samples_per_second': 1489.384, 'eval_steps_per_second': 23.341, 'epoch': 10.0}\n","***** eval for sa metrics *****\n","  epoch                   =       10.0\n","  eval_accuracy           =     0.9373\n","  eval_f1                 =       0.94\n","  eval_loss               =     0.2616\n","  eval_precision          =      0.954\n","  eval_recall             =     0.9264\n","  eval_runtime            = 0:00:02.01\n","  eval_samples            =       2999\n","  eval_samples_per_second =   1489.384\n","  eval_steps_per_second   =     23.341\n","--------------------------------------------\n","\u001b[32m2023-08-21 08:57:10.794\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m510\u001b[0m - \u001b[32m\u001b[1mExecution time: 00:04:27\u001b[0m\n"]}]},{"cell_type":"markdown","source":["### **ViDeBERTa-base**"],"metadata":{"id":"DK5mJ2b6Ihi1"}},{"cell_type":"code","source":[],"metadata":{"id":"7trBx1YYIQAO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **PhoBERT-base-v2**"],"metadata":{"id":"RK4k909bIhq4"}},{"cell_type":"code","source":[],"metadata":{"id":"2abBJuw7IQes"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **ZSL**"],"metadata":{"id":"Y0TMrXhPQkuC"}},{"cell_type":"markdown","source":["### **ViDeBERTa-xmall**"],"metadata":{"id":"fW8OhG1yQm_i"}},{"cell_type":"code","source":[],"metadata":{"id":"HBs4XNZzQuR6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **ViDeBERTa-base**"],"metadata":{"id":"vr2znLagQnfi"}},{"cell_type":"code","source":[],"metadata":{"id":"T-tFHIY4Qu7T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **PhoBERT-base-v2**"],"metadata":{"id":"_bz-a8hPQnuS"}},{"cell_type":"code","source":[],"metadata":{"id":"ZZUiV_3tQvbT"},"execution_count":null,"outputs":[]}]}